---
title: "CISS-VAE Vignette"
format: html
---

# Overview

The Clustering-Informed Shared-Structure Variational Autoencoder (CISS-VAE) is a flexible deep learning model for missing data imputation, especially useful when the missingness mechanism may depend on unobserved or latent variablesâ€”also known as Missing Not At Random (MNAR).

Unlike traditional imputation models that assume data are Missing Completely At Random (MCAR) or Missing At Random (MAR), CISS-VAE:   
- Learns patterns of missingness via unsupervised clustering.  
- Builds a shared and cluster-specific encoder-decoder structure.  
- Performs variational inference to model the joint distribution of observed and missing values.  
- Supports a validation-masked impute-refit loop for better generalization.


There are two ways to run the CISS-VAE process. If you know what model parameters you want to use, you can use the `run_cissvae` function to run the model once for the given set of parameters. If you want to tune the model instead, you can use `autotune`.

# Installation

The CISS-VAE package is currently available for python, with an R package to be released soon. It can be installed from either github or PyPI. 

```{.bash}
# From PyPI
pip install ciss-vae

# From GitHub (latest development version)
pip install git+https://github.com/your-repo/ciss-vae.git

```
# Quickstart 

If you already know what parameters you want for your model (or do not want to use the `autotune` function), you can use the `run_cissvae` function. 

Your dataset should be one of the following:

    - A Pandas DataFrame  

    - A NumPy array  

    - A PyTorch tensor  

Missing values should be represented using np.nan or None.

```{python}
import pandas as pd
from ciss_vae.utils.run_cissvae import run_cissvae

# optional, display vae architecture
from ciss_vae.utils.helpers import plot_vae_architecture

data = pd.read_csv("/data/test_data.csv")

clusters = data.clusters
data = data.drop(columns = ["clusters", "Unnamed: 0"])

imputed_data, vae = run_cissvae(data = data,
## Dataset params
    val_percent = 0.1, ## Explain! Frac hold out for validation
    replacement_value = 0.0, 
    columns_ignore = data.columns[:5], ## columns to ignore when selecting validation dataset (and clustering if you do not provide clusters). For example, demographic columns with no missingness.
    print_dataset = True, 
## Cluster params
    clusters = None, ## Where your cluster list goes. If none, will do clustering for you  
    n_clusters = None, ## If you want run_cissvae to do clustering and you know how many clusters your data should have
    cluster_selection_epsilon = 0.25, ## Cluster Selection Epsilon for HDBSCAN (link)
    seed = 42,
## VAE model params
    hidden_dims = [150, 120, 60], ## Dimensions of hidden layers, in order. One number per layer. 
    latent_dim = 15, ## Dimensions of latent embedding
    layer_order_enc = ["unshared", "unshared", "unshared"], ## order of shared vs unshared layers for encode (can use u or s instead of unshared, shared)
    layer_order_dec=["shared", "shared",  "shared"],  ## order of shared vs unshared layers for decode
    latent_shared=False, 
    output_shared=False, 
    batch_size = 4000, ## batch size for data loader
    return_model = True, ## if true, outputs imputed dataset and model, otherwise just outputs imputed dataset. Set to true to return model for `plot_vae_architecture`
## Initial Training params
    epochs = 1000, ## default 
    initial_lr = 0.01, ## default
    decay_factor = 0.999, ## default, factor learning rate is multiplied by after each epoch, prevents overfitting
    beta= 0.001, ## default
    device = None, ## If none, will use gpu if available, cpu if not. See torch.devices for info (link)
## Impute-refit loop params
    max_loops = 100, ## max number of refit loops
    patience = 2, ## number of loops to check after best_dataset updated. Can increase to avoid local extrema
    epochs_per_loop = None, ## If none, same as epochs
    initial_lr_refit = None, ## If none, picks up from end of initial training
    decay_factor_refit = None, ## If none, same as decay_factor
    beta_refit = None, ## if none, same as beta
    verbose = False
)

## OPTIONAL - PLOT VAE ARCHITECTURE
plot_vae_architecture(model = vae,
                        title = None, ## Set title of plot
                        ## Colors below are default
                        color_shared = "skyblue", 
                        color_unshared ="lightcoral",
                        color_latent = "gold", # xx fix
                        color_input = "lightgreen",
                        color_output = "lightgreen",
                        figsize=(16, 8))

```

# Hyperparameter Tuning with Optuna

The `autotune` function lets you tune the model's hyperparameters with optuna to get the best possible model. 

## Dataset Preparation

Your dataset should be one of the following:

    - A Pandas DataFrame  

    - A NumPy array  

    - A PyTorch tensor  

Missing values should be represented using np.nan or None.

Once your dataset is loaded, the first step is to identify patterns of missingness using clustering.

## Clustering on missingness pattern

Before fitting the model, the dataset is clustered based on its missingness pattern (i.e., which variables are missing in each observation).

You can use the built-in function:  

```{python}
from ciss_vae.utils import cluster_on_missing

clusters = cluster_on_missing(data, cols_ignore=None, n_clusters=None, seed=42)
```

This function uses HDBSCAN clustering to detect structure in binary missingness masks, and will automatically determine the number of clusters if not specified. If n_clusters is specified, uses KMeans.

<b>Options:</b>
- cols_ignore: list of columns to exclude when computing the missingness pattern.

- n_clusters: set this to use K-Means instead of nonparametric clustering.

You should store your cluster labels separately for input into the model constructor.

# Creating a `ClusterDataset` object

Once you've computed the cluster labels, you'll convert your dataset into a `ClusterDataset`.


```{python}
from ciss_vae.classes.cluster_dataset import ClusterDataset
from ciss_vae.training.autotune import SearchSpace, autotune

dataset = ClusterDataset(data = data,
cluster_labels = clusters,
val_percent = 0.1, ## 10% non-missing data is default.
replacement_value = 0, ## value to replace all missing data with before running model. Could be set to 0 or random
columns_ignore = data.columns[:5] ## Tells ClusterDataset not to hold out entries demographic columns for validation
)

```

## Create a SearchSpace object:

In the SearchSpace object, define the search space for each hyperparameter. Each of the parameters in `SearchSpace()` can be set as either tunable or non-tunable. 

Types of parameters:     
    - (min, max, step) -> creates a range
    - [a, b, c] -> select value from list
    - x -> set param as non-tunable


```{python}
## These are the default parameters. Please note these parameters may not be best for all datasets depending on size and complexity.

searchspace = SearchSpace(
                 num_hidden_layers=(1, 4), ## Set number of hidden layers
                 hidden_dims=[64, 512],
                 latent_dim=[10, 100],
                 latent_shared=[True, False],
                 output_shared=[True,False],
                 lr=(1e-4, 1e-3),
                 decay_factor=(0.9, 0.999),
                 beta=0.01,
                 num_epochs=1000,
                 batch_size=64,
                 num_shared_encode=[0, 1, 3],
                 num_shared_decode=[0, 1, 3],
                 refit_patience=2,
                 refit_loops=100,
                 epochs_per_loop = 1000,
                 reset_lr_refit = [True, False])
```

## Run the `autotune` function:

Once the search space is set, the autotune function can be run. 

```{python}

best_imputed_df,  best_model, study, results_df = autotune(
    search_space = searchspace,
    train_dataset = dataset,                   # ClusterDataset object
    save_model_path=None,
    save_search_space_path=None,
    n_trials=20,
    study_name="vae_autotune",                 # Default study name
    device_preference="cuda",
    show_progress=False,                       # Show progress bar for training
    optuna_dashboard_db=None,                  # If using optuna dashboard set db location here
    load_if_exists=True,                       # If using optuna dashboard, if study by 'study_name' already exists, will load that study
    seed = 42,                                 # Sets seed for random order of shared/unshared layers
)
```

## (optional) Using Optuna Dashboard

You can use optuna dashboard (https://optuna-dashboard.readthedocs.io/en/stable/getting-started.html) to visualize the importance of your tuning parameters. If you use VSCode or Positron there is an extension for viewing optuna dashboards in your development environment. 

To use optuna dashboard, set your database in the autotune function. You can have multiple autotune 'studies' in the same database and compare them. 

```{python}
best_imputed_df,  best_model, study, results_df = autotune(
    search_space = searchspace,
    train_dataset = dataset,                   # ClusterDataset object
    save_model_path=None,
    save_search_space_path=None,
    n_trials=20,
    study_name="vae_autotune",                 # Default study name
    device_preference="cuda",
    show_progress=False,                       # Show progress bar for training
    optuna_dashboard_db="sqlite:///db.sqlite3",                  # If using optuna dashboard set db location here
    load_if_exists=True,
    seed = 42,
)
```

# Saving and loading models

## Saving

If you want to save your model and load it later, there are two options. 

To save the model weights after training:
```{python}
## assuming your trained model is called 'model'
import torch

torch.save(model.state_dict(), "trained_vae.pt")

```

If you want to save the entire model (not usually recommended):


```{python}
torch.save(model, "trained_vae_full.pt")
```

## Loading a Model

To reload the model for imputation or further training:
1. Re-create the model architecture with the same settings used during training  
2. Load the saved weights  


```{python}
from ciss_vae.classes.vae import CISSVAE

# 1. Define the architecture (must match the saved model!)
model = CISSVAE(
    input_dim=...,
    hidden_dims=[...],
    layer_order_enc=[...],
    layer_order_dec=[...],
    latent_shared=...,
    num_clusters=...,
    latent_dim=...,
    output_shared=...
)
model.load_state_dict(torch.load("trained_vae.pt"))



## optional to get imputed dataset. 
from ciss_vae.utils.helpers import get_imputed_df
from torch.utils.data import DataLoader

## assuming dataset is a ClusterDataset
data_loader =  DataLoader(dataset, batch_size=4000)

imputed_df = get_imputed_df(model, data_loader)

```